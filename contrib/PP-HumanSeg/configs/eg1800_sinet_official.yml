batch_size: 18    # total batch size: 36
iters: 25000

train_dataset:
  type: Dataset
  dataset_root: data/EG1800
  train_path: data/EG1800/train.txt
  num_classes: 2
  transforms:
    - type: Resize
      target_size: [224, 224]
    - type: ResizeStepScaling
      scale_step_size: 0
    - type: RandomRotation
    - type: RandomPaddingCrop
      crop_size: [224, 224]
    - type: RandomHorizontalFlip
    - type: RandomDistort
    - type: RandomBlur
      prob: 0.3
    - type: Normalize
  mode: train

val_dataset:
  type: Dataset
  dataset_root: data/EG1800
  val_path: data/EG1800/val.txt
  num_classes: 2
  transforms:
    - type: Resize
      target_size: [224, 224]
    - type: Normalize
  mode: val

export:
  transforms:
    - type: Resize
      target_size: [224, 224]
    - type: Normalize

optimizer:
  type: adam
  weight_decay: 2.0e-4

lr_scheduler:
  type: PolynomialDecay
  learning_rate: 7.5e-3
  end_lr: 0
  power: 0.9
  warmup_iters: 1000
  warmup_start_lr: 1.0e-6

loss:
  types:
    - type: MixedLoss
      losses:
        - type: CrossEntropyLoss
        - type: LovaszSoftmaxLoss
      coef: [0.8, 0.2]
  coef: [1]

model:
  type: SINet
